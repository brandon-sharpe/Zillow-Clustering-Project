{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fda3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# env import\n",
    "from env import host, user, password\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d935914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# env import\n",
    "from env import host, user, password\n",
    "\n",
    "# train test split import\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def get_connection(db, username=user, host=host, password=password):\n",
    "    '''\n",
    "    Creates a connection URL\n",
    "    '''\n",
    "    return f'mysql+pymysql://{username}:{password}@{host}/{db}'\n",
    "    \n",
    "def new_zillow_data():\n",
    "    '''\n",
    "    Returns zillow into a dataframe\n",
    "    '''\n",
    "    sql_query = '''  SELECT *\n",
    "    FROM properties_2017\n",
    "    LEFT OUTER JOIN airconditioningtype \n",
    "    USING (airconditioningtypeid)\n",
    "    LEFT OUTER JOIN architecturalstyletype\n",
    "    USING (architecturalstyletypeid)\n",
    "    LEFT OUTER JOIN buildingclasstype \n",
    "    USING (buildingclasstypeid)\n",
    "    LEFT OUTER JOIN heatingorsystemtype\n",
    "    USING (heatingorsystemtypeid)\n",
    "    LEFT OUTER JOIN predictions_2017\n",
    "    USING (id)\n",
    "    INNER JOIN (\n",
    "    SELECT id, MAX(transactiondate) as last_trans_date \n",
    "    FROM predictions_2017\n",
    "    GROUP BY id\n",
    "    ) predictions ON predictions.id = properties_2017.id AND predictions_2017.transactiondate = predictions.last_trans_date\n",
    "    LEFT OUTER JOIN propertylandusetype\n",
    "    USING(propertylandusetypeid)\n",
    "    LEFT OUTER JOIN storytype\n",
    "    ON storytype.storytypeid = properties_2017.storytypeid\n",
    "    LEFT OUTER JOIN typeconstructiontype\n",
    "    ON typeconstructiontype.typeconstructiontypeid = properties_2017.typeconstructiontypeid\n",
    "    JOIN unique_properties\n",
    "    ON unique_properties.parcelid = properties_2017.parcelid\n",
    "    WHERE latitude IS NOT NULL and longitude IS NOT NULL; '''\n",
    "    df = pd.read_sql(sql_query, get_connection('zillow'))\n",
    "    return df \n",
    "\n",
    "def get_zillow_data():\n",
    "    '''get connection, returns zillow into a dataframe and creates a csv for us'''\n",
    "    if os.path.isfile('zillow.csv'):\n",
    "        df = pd.read_csv('zillow.csv', index_col=0)\n",
    "    else:\n",
    "        df = new_zillow_data()\n",
    "        df.to_csv('zillow.csv')\n",
    "    return df\n",
    "\n",
    "def drop_nulls(df, prop_req_col = .5 , prop_req_row = .5, inplace = True):\n",
    "    '''Drops colums and rows with more than 50 % null values'''\n",
    "    threshold = int(prop_req_col * len(df.index)) \n",
    "    df.dropna(axis = 1, thresh = threshold, inplace = True)\n",
    "    threshold = int(prop_req_row * len(df.columns)) \n",
    "    df.dropna(axis = 0, thresh = threshold, inplace = True)\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and returns that dataframe\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[f'{col}'].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        return df[(df[f'{col}'] > lower_bound) & (df[f'{col}'] < upper_bound)]\n",
    "    \n",
    "def get_counties(df):\n",
    "    '''\n",
    "    This function will create dummy variables out of the original fips column. \n",
    "    And return a dataframe with all of the original columns except regionidcounty.\n",
    "    We will keep fips column for data validation after making changes. \n",
    "    New columns added will be 'LA', 'Orange', and 'Ventura' which are boolean \n",
    "    The fips ids are renamed to be the name of the county each represents. \n",
    "    '''\n",
    "    # create dummy vars of fips id\n",
    "    county_df = pd.get_dummies(df.fips)\n",
    "    # rename columns by actual county name\n",
    "    county_df.columns = ['LA', 'Orange', 'Ventura']\n",
    "    # concatenate the dataframe with the 3 county columns to the original dataframe\n",
    "    df_dummies = pd.concat([df, county_df], axis = 1)\n",
    "    # drop regionidcounty and fips columns\n",
    "    df_dummies = df_dummies.drop(columns = ['regionidcounty'])\n",
    "    #county column with which county the property is located in\n",
    "    df_dummies['county'] = df_dummies.fips.apply(lambda x: 'Orange' if x == 6059.0 else 'Los angeles' if x == 6037.0 else 'Ventura')\n",
    "    #drop fips with county and encoded counties i wont need fips anymore\n",
    "    df_dummies = df_dummies.drop(columns=['fips'])\n",
    "    return df_dummies\n",
    "\n",
    "def remove_outliers_further(df):\n",
    "    '''\n",
    "    remove outliers in bed(less than zero), bath(less than zero), square feet, & acres\n",
    "    '''\n",
    "\n",
    "    return df[((df.bathroomcnt <= 7) & (df.bedroomcnt <= 7) & \n",
    "               (df.bathroomcnt > 0) & \n",
    "               (df.bedroomcnt > 0) & \n",
    "               (df.acres < 15) &\n",
    "               (df.calculatedfinishedsquarefeet < 10000)\n",
    "              )]\n",
    "\n",
    "def train_validate_test_split(df, target, seed=66):\n",
    "    '''\n",
    "    This function takes in a dataframe, the name of the target variable\n",
    "    (for stratification purposes), and an integer for a setting a seed\n",
    "    and splits the data into train, validate and test. \n",
    "    The function returns, in this order, train, validate and test dataframes. \n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=0.2, \n",
    "                                            random_state=seed)\n",
    "    train, validate = train_test_split(train_validate, test_size=0.3, \n",
    "                                       random_state=seed)\n",
    "    \n",
    "    # split train into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_train = train.drop(columns=[target])\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # split validate into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_validate = validate.drop(columns=[target])\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # split test into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_test = test.drop(columns=[target])\n",
    "    y_test = test[target]\n",
    "    \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prep_zillow(df):\n",
    "    '''Removes all outlieirs from the function via remove_outliers and remove_outliers_2,\n",
    "    drops all irelevant columns, drops items from column and rows with less than 50% value. Fills remaining null values, \n",
    "    Drops duplicated columns brought in from MySQL. '''\n",
    "    \n",
    "    # brings in data from sql or csv file\n",
    "    df = get_zillow_data()\n",
    "    \n",
    "    # Drops duplicated columns from MySql\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    # Ensures we are only bringing in single use properties\n",
    "    single_use_codes = [261, 262, 263, 264, 265, 268, 273,275, 276, 279]\n",
    "    df = df[df['propertylandusetypeid'].isin(single_use_codes)]\n",
    "    \n",
    "    # Drops null rows and columns that have less than have more nulls than threshhold (50%)\n",
    "    df = drop_nulls(df, prop_req_col = .5 , prop_req_row = .5, inplace = True)\n",
    "    \n",
    "\n",
    "    dropcols = ['id',\n",
    "            'heatingorsystemtypeid',\n",
    "            'propertycountylandusecode',\n",
    "            'buildingqualitytypeid',\n",
    "            'rawcensustractandblock',\n",
    "            'unitcnt',\n",
    "            'propertyzoningdesc',\n",
    "            'heatingorsystemdesc',\n",
    "            'censustractandblock',\n",
    "            'calculatedbathnbr',\n",
    "            'finishedsquarefeet12',\n",
    "            'fullbathcnt',\n",
    "            'assessmentyear',\n",
    "            'propertylandusetypeid',\n",
    "            'parcelid.1',\n",
    "            'id.1',\n",
    "            'parcelid.2',\n",
    "            'roomcnt',\n",
    "            'last_trans_date']\n",
    "    \n",
    "    # Drops columns I have deemed irellivant\n",
    "     # - id because its a usless and duplicated\n",
    "     # - heatingorsystemtypeid because it was missing about 20k values to much to fill\n",
    "     # - heatingorsystemdesc because it was missing about 20k values to much to fill\n",
    "     # - propertylandusetypeid is useless to me after the dropping irrelevant data earlier\n",
    "     # - buildingqualitytypeid because it was missing about 20k values to much to fill\n",
    "     # - rawcensustractandblock useless data to me\n",
    "     # - unitcnt is useless to me after the dropping irrelevant data earlier\n",
    "     # - propertyzoningdesc because it was missing about 20k values to much to fill\n",
    "     # - censustractandblock isn't useful to me\n",
    "     # - calculatedbathnbr data is inconsistent \n",
    "     # - finishedsquarefeet12 calculatedsquarefeet is a better metric\n",
    "     # - fullbathcnt redundant to bathroom count\n",
    "     # - assessmentyear values are all 2016\n",
    "     # - propertylandusetypeid because the data was filtered already. \n",
    "    df = df.drop(columns=dropcols)\n",
    "    \n",
    "    \n",
    "    # Creats 3 new boolean columns out of fips labeling counties\n",
    "    df = get_counties(df)\n",
    "    \n",
    "    \n",
    "    # Filling nulls in yearbuilt with 2017\n",
    "    df['yearbuilt'].fillna(2017, inplace = True)\n",
    "    # Fillin nulls in regioncityid with the mode, which leads the rest by almost 10k\n",
    "    df['regionidcity'].fillna(df.regionidcity.mode, inplace = True )\n",
    "    # Fillin nulls in regionidzip with the mode 63 values\n",
    "    df['regionidzip'].fillna(df.regionidzip.mode, inplace = True )\n",
    "    # Dropped about a thousand values here no eal good way to determine land size\n",
    "    df.dropna(subset=['lotsizesquarefeet'], inplace = True)\n",
    "    # Drop remaining nulls\n",
    "    df = df.dropna()\n",
    "    \n",
    "    \n",
    "    # Added an acres column, When I previously explored this data I had noticed some anomolies\n",
    "    df['acres'] = df.lotsizesquarefeet/43560\n",
    "    \n",
    "    # Removes properties with 0 beds or baths, properties with greater than 15 acres of property, more than 6 bedrooms or bathrroms and mre than 10000 sqft\n",
    "    df = remove_outliers_further(df)\n",
    "    \n",
    "    #rename the columns\n",
    "    df = df.rename(columns={\n",
    "                            'calculatedfinishedsquarefeet': 'sqft',\n",
    "                            'bathroomcnt': 'baths',\n",
    "                            'bedroomcnt': 'beds',\n",
    "                            'taxvaluedollarcnt':'tax_value',\n",
    "                            'yearbuilt':'year_built',\n",
    "                            'taxamount': 'tax_amount'\n",
    "        \n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a88742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3357: DtypeWarning: Columns (61,62,71,73) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (61,62,71,73) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df =prep_zillow(get_zillow_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de7d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55513, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5205df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407b6fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>baths</th>\n",
       "      <th>beds</th>\n",
       "      <th>sqft</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>year_built</th>\n",
       "      <th>structuretaxvaluedollarcnt</th>\n",
       "      <th>tax_value</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>tax_amount</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "      <th>propertylandusedesc</th>\n",
       "      <th>LA</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Ventura</th>\n",
       "      <th>county</th>\n",
       "      <th>acres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11324547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3633.0</td>\n",
       "      <td>34560018.0</td>\n",
       "      <td>-118169806.0</td>\n",
       "      <td>9826.0</td>\n",
       "      <td>40227.0</td>\n",
       "      <td>97329.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>222321.0</td>\n",
       "      <td>296425.0</td>\n",
       "      <td>74104.0</td>\n",
       "      <td>6941.39</td>\n",
       "      <td>0.042463</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Los angeles</td>\n",
       "      <td>0.225574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>11585547</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2077.0</td>\n",
       "      <td>34012977.0</td>\n",
       "      <td>-118479243.0</td>\n",
       "      <td>6490.0</td>\n",
       "      <td>26964.0</td>\n",
       "      <td>96152.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>210192.0</td>\n",
       "      <td>646760.0</td>\n",
       "      <td>436568.0</td>\n",
       "      <td>7924.68</td>\n",
       "      <td>-0.040807</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Los angeles</td>\n",
       "      <td>0.148990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parcelid  baths  beds    sqft    latitude    longitude  lotsizesquarefeet  \\\n",
       "20  11324547    2.0   4.0  3633.0  34560018.0 -118169806.0             9826.0   \n",
       "33  11585547    2.0   3.0  2077.0  34012977.0 -118479243.0             6490.0   \n",
       "\n",
       "   regionidcity regionidzip  year_built  structuretaxvaluedollarcnt  \\\n",
       "20      40227.0     97329.0      2005.0                    222321.0   \n",
       "33      26964.0     96152.0      1926.0                    210192.0   \n",
       "\n",
       "    tax_value  landtaxvaluedollarcnt  tax_amount  logerror transactiondate  \\\n",
       "20   296425.0                74104.0     6941.39  0.042463      2017-01-02   \n",
       "33   646760.0               436568.0     7924.68 -0.040807      2017-01-02   \n",
       "\n",
       "          propertylandusedesc  LA  Orange  Ventura       county     acres  \n",
       "20  Single Family Residential   1       0        0  Los angeles  0.225574  \n",
       "33  Single Family Residential   1       0        0  Los angeles  0.148990  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324e2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb27182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_use_codes = [261, 262, 263, 264, 265, 268, 273,275, 276, 279]\n",
    "df = df[df['propertylandusetypeid'].isin(single_use_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dda3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_nulls(df, prop_req_col = .5 , prop_req_row = .5, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb408a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['id','propertycountylandusecode','rawcensustractandblock','unitcnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 1.5, ['calculatedfinishedsquarefeet', 'bedroomcnt', 'bathroomcnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_zillow(df):\n",
    "    '''Removes all outlieirs from the function via remove_outliers and remove_outliers_2,\n",
    "    drops all irelevant columns, drops items from column and rows with less than 50% value. Fills remaining null values, \n",
    "    Drops duplicated columns brought in from MySQL. '''\n",
    "    \n",
    "    # brings in data from sql or csv file\n",
    "    df = get_zillow_data()\n",
    "    \n",
    "    # Drops duplicated columns from MySql\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    # Ensures we are only bringing in single use properties\n",
    "    single_use_codes = [261, 262, 263, 264, 265, 268, 273,275, 276, 279]\n",
    "    df = df[df['propertylandusetypeid'].isin(single_use_codes)]\n",
    "    \n",
    "    # Drops null rows and columns that have less than have more nulls than threshhold (50%)\n",
    "    df = drop_nulls(df, prop_req_col = .5 , prop_req_row = .5, inplace = True)\n",
    "    \n",
    "\n",
    "    dropcols = ['id',\n",
    "            'heatingorsystemtypeid',\n",
    "            'propertycountylandusecode',\n",
    "            'buildingqualitytypeid',\n",
    "            'rawcensustractandblock',\n",
    "            'unitcnt',\n",
    "            'propertyzoningdesc',\n",
    "            'heatingorsystemdesc',\n",
    "            'censustractandblock',\n",
    "            'calculatedbathnbr',\n",
    "            'finishedsquarefeet12',\n",
    "            'fullbathcnt',\n",
    "            'assessmentyear',\n",
    "            'propertylandusetypeid']\n",
    "    \n",
    "    # Drops columns I have deemed irellivant\n",
    "     # - id because its a usless and duplicated\n",
    "     # - heatingorsystemtypeid because it was missing about 20k values to much to fill\n",
    "     # - heatingorsystemdesc because it was missing about 20k values to much to fill\n",
    "     # - propertylandusetypeid is useless to me after the dropping irrelevant data earlier\n",
    "     # - buildingqualitytypeid because it was missing about 20k values to much to fill\n",
    "     # - rawcensustractandblock useless data to me\n",
    "     # - unitcnt is useless to me after the dropping irrelevant data earlier\n",
    "     # - propertyzoningdesc because it was missing about 20k values to much to fill\n",
    "     # - censustractandblock isn't useful to me\n",
    "     # - calculatedbathnbr data is inconsistent \n",
    "     # - finishedsquarefeet12 calculatedsquarefeet is a better metric\n",
    "     # - fullbathcnt redundant to bathroom count\n",
    "     # - assessmentyear values are all 2016\n",
    "     # - propertylandusetypeid because the data was filtered already. \n",
    "    df = df.drop(columns=dropcols)\n",
    "    \n",
    "    \n",
    "    # Creats 3 new boolean columns out of fips labeling counties\n",
    "    df = get_counties(df)\n",
    "    \n",
    "    \n",
    "    # Filling nulls in yearbuilt with 2017\n",
    "    df['yearbuilt'].fillna(2017, inplace = True)\n",
    "    # Fillin nulls in regioncityid with the mode, which leads the rest by almost 10k\n",
    "    df['regionidcity'].fillna(df.regionidcity.mode, inplace = True )\n",
    "    # Fillin nulls in regionidzip with the mode 63 values\n",
    "    df['regionidzip'].fillna(df.regionidzip.mode, inplace = True )\n",
    "    # Dropped about a thousand values here no eal good way to determine land size\n",
    "    df.dropna(subset=['lotsizesquarefeet'], inplace = True)\n",
    "    # Drop remaining nulls\n",
    "    df = df.dropna()\n",
    "    \n",
    "    \n",
    "    # Added an acres column, When I previously explored this data I had noticed some anomolies\n",
    "    df['acres'] = df.lotsizesquarefeet/43560\n",
    "    \n",
    "    # Removes properties with 0 beds or baths, properties with greater than 15 acres of property, more than 6 bedrooms or bathrroms and mre than 10000 sqft\n",
    "    df = remove_outliers_features(df)\n",
    "    \n",
    "    #rename the columns\n",
    "    df = df.rename(columns={\n",
    "                            'calculatedfinishedsquarefeet': 'sqft',\n",
    "                            'bathroomcnt': 'baths',\n",
    "                            'bedroomcnt': 'beds',\n",
    "                            'taxvaluedollarcnt':'tax_value',\n",
    "                            'yearbuilt':'year_built',\n",
    "                            'taxamount': 'tax_amount'\n",
    "        \n",
    "    })\n",
    "    \n",
    "    # Splits data into train, validate, test, X_train, y_train, X_validate, y_validate, X_test, and y_test\n",
    "    train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test_split(df,'logerror', seed=66)\n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf57d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    '''\n",
    "    summarize will take in a single argument (a pandas dataframe) \n",
    "    and output to console various statistics on said dataframe, including:\n",
    "    # .head()\n",
    "    # .info()\n",
    "    # .describe()\n",
    "    # value_counts()\n",
    "    # observation of nulls in the dataframe\n",
    "    '''\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe head: ')\n",
    "    print(df.head(3).to_markdown())\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe info: ')\n",
    "    print(df.info())\n",
    "    print('=====================================================\\n\\n')\n",
    "    print('Dataframe Description: ')\n",
    "    print(df.describe().to_markdown())\n",
    "    num_cols = [col for col in df.columns if df[col].dtype != 'O']\n",
    "    cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "    print('=====================================================')\n",
    "    print('DataFrame value counts: ')\n",
    "    for col in df.columns:\n",
    "        if col in cat_cols:\n",
    "            print(df[col].value_counts())\n",
    "        else:\n",
    "            print(df[col].value_counts(bins=10, sort=False))\n",
    "    print('=====================================================')\n",
    "    print('nulls in dataframe by column: ')\n",
    "    print(nulls_by_col(df))\n",
    "    print('=====================================================')\n",
    "    print('nulls in dataframe by row: ')\n",
    "    print(nulls_by_row(df))\n",
    "    print('=====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_counties(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f144bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.propertylandusetypeid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192be05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['id',\n",
    "            'heatingorsystemtypeid',\n",
    "            'propertycountylandusecode',\n",
    "            'buildingqualitytypeid',\n",
    "            'rawcensustractandblock',\n",
    "            'unitcnt','propertyzoningdesc',\n",
    "            'heatingorsystemdesc',\n",
    "            'censustractandblock',\n",
    "            'calculatedbathnbr',\n",
    "            'finishedsquarefeet12',\n",
    "            'fullbathcnt',\n",
    "            'assessmentyear',\n",
    "            'propertylandusetypeid']\n",
    "df = df.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97667879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51af805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.regionidzip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling nulls in yearbuilt with 2017\n",
    "df['yearbuilt'].fillna(2017, inplace = True)\n",
    "# Fillin nulls in regioncityid with the mode, which leads the rest by almost 10k\n",
    "df['regionidcity'].fillna(df.regionidcity.mode, inplace = True )\n",
    "# Fillin nulls in regionidzip with the mode 63 values\n",
    "df['regionidzip'].fillna(df.regionidzip.mode, inplace = True )\n",
    "# Dropped about a thousand values here no eal good way to determine land size\n",
    "df.dropna(subset=['lotsizesquarefeet'], inplace = True)\n",
    "# Drop remaining nulls\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4aac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.regionidzip.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23160d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining nulls\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test_split(df,'logerror', seed=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train.shape,validate.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e95c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b63493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
